// src/core/histogram.ts
import { glob } from 'fast-glob';
import path from 'path';
import fs from 'fs-extra';
import { colord, extend } from 'colord';
import namesPlugin from 'colord/plugins/names';
import { TokenMeta, TokenType, FenceConfig } from '@/types';

extend([namesPlugin]);

const HEX_COLOR_REGEX = /#(?:[0-9a-fA-F]{3}){1,2}(?![0-9a-fA-F])/g;

// --- 1. å®šä¹‰æ¢æµ‹å™¨æ¥å£ (Detector Interface) ---
interface StyleDetector {
  type: TokenType;
  regex: RegExp;
  // å¯é€‰ï¼šå½’ä¸€åŒ–å‡½æ•° (ä¾‹å¦‚æŠŠ p-4 å’Œ pt-4 éƒ½å½’ä¸º spacing: 4)
  normalize?: (match: string) => string | null; 
}

// --- 2. é…ç½®æ¢æµ‹è§„åˆ™ (The Rules) ---
const DETECTORS: StyleDetector[] = [
  {
    type: 'color',
    // åŒ¹é… Hex, RGB, RGBA
    regex: /#([0-9a-f]{3}){1,2}\b|rgb\(\s*\d+\s*,\s*\d+\s*,\s*\d+\s*\)|rgba\([^)]+\)/gi,
    normalize: (val) => {
      const c = colord(val);
      return c.isValid() ? c.toHex() : null;
    }
  },
  {
    type: 'spacing',
    // ğŸŒŸ é‡ç‚¹: åŒæ—¶æ•è· Tailwind Utility å’Œ Raw CSS Unit
    // æ•è·: p-4, m-2, gap-x-4, w-full, h-10
    // æ•è·: 16px, 1rem
    regex: /\b([pmWH][tbrlxy]?|gap(-[xy])?|space-[xy])-([0-9.]+|px)\b|\b\d+(\.\d+)?(px|rem|em)\b/g,
    normalize: (val) => val // æš‚æ—¶ç›´æ¥ç»Ÿè®¡åŸå§‹å€¼ï¼Œçœ‹å“ªä¸ªç”¨å¾—å¤š
  },
  {
    type: 'radius',
    // æ•è·: rounded-lg, rounded-md, rounded-tr-xl
    // æ•è·: 8px (å¦‚æœåœ¨ border-radius ä¸Šä¸‹æ–‡æ¯”è¾ƒéš¾åŒºåˆ†ï¼Œè¿™é‡Œå…ˆç²—ç•¥æ•è·)
    regex: /\brounded(-[tbrl][r l]?)?-(sm|md|lg|xl|2xl|3xl|full|none)\b/g,
    normalize: (val) => val
  },
  {
    type: 'typography',
    // æ•è·: text-xl, text-sm, font-bold
    regex: /\b(text-(xs|sm|base|lg|xl|2xl|3xl|4xl|5xl)|font-(thin|light|normal|medium|bold|black))\b/g,
    normalize: (val) => val
  },
  {
     type: 'shadow',
     regex: /\bshadow-(sm|md|lg|xl|2xl|inner|none)\b/g,
     normalize: (val) => val
  }
];

// --- 3. é€šç”¨æ‰«æé€»è¾‘ ---
export async function scanShadowTokens(root: string): Promise<TokenMeta[]> {
  // 1. ğŸŒŸ æ–°å¢: è¯»å– Config (å¤ç”¨ scanner çš„é€»è¾‘)
  const configPath = path.join(root, '.fence/fence.config.json');
  let includePatterns = ['src/**/*.{ts,tsx,js,jsx}'];
  let excludePatterns = ['**/node_modules/**'];

  if (await fs.pathExists(configPath)) {
    try {
      const config: FenceConfig = await fs.readJSON(configPath);
      if (config.scan?.include) includePatterns = config.scan.include;
      if (config.scan?.exclude) excludePatterns = config.scan.exclude;
    } catch (e) {
      // ignore config error
    }
  }

  // 2. ğŸŒŸ ä¿®æ”¹: ä½¿ç”¨ Config ä¸­çš„è·¯å¾„
  const files = await glob(includePatterns, {
    cwd: root,
    absolute: true,
    ignore: excludePatterns,
    // å…è®¸æ‰«æ . å¼€å¤´çš„ç›®å½• (å¦‚ .storybook ç­‰ï¼Œå¦‚æœç”¨æˆ·includeäº†)
    dot: true 
  });

  const tokenMap = new Map<string, TokenMeta>();

  // 3. éå†æ–‡ä»¶ (é€»è¾‘ä¿æŒä¸å˜)
  for (const file of files) {
    const content = await fs.readFile(file, 'utf-8');
    
    // ... åŸæœ‰çš„ extractTokensFromText é€»è¾‘ ...
    extractTokensFromText(content, 'color', HEX_COLOR_REGEX, file, tokenMap);
    // extractTokensFromText(content, 'spacing', TAILWIND_SPACING_REGEX, file, tokenMap); // å¦‚æœä½ æœ‰è¿™ä¸ª
  }

  // è½¬æ¢ Map åˆ° Array
  return Array.from(tokenMap.values()).sort((a, b) => b.count - a.count);
}

// ... extractTokensFromText ä¿æŒä¸å˜ ...
function extractTokensFromText(
  content: string, 
  type: 'color' | 'spacing', 
  regex: RegExp, 
  file: string, 
  map: Map<string, TokenMeta>
) {
  let match;
  while ((match = regex.exec(content)) !== null) {
    const value = match[0].toLowerCase(); // å½’ä¸€åŒ–
    const relativePath = path.basename(file); // ç®€åŒ–è·¯å¾„è®°å½•

    if (!map.has(value)) {
      map.set(value, { 
        type, 
        value, 
        count: 0, 
        usedBy: [],
        source: 'scan' 
      });
    }

    const token = map.get(value)!;
    token.count++;
    if (!token.usedBy?.includes(relativePath)) {
      token.usedBy?.push(relativePath);
    }
  }
}